{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- simulated data\n",
    "- transformer + dataset construction\n",
    "- pretrain\n",
    "\n",
    "- analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_lowercase\n",
    "\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    # CONFIG_MAPPING,\n",
    "    # MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    # AutoConfig,\n",
    "    # AutoModelForCausalLM,\n",
    "    # AutoTokenizer,\n",
    "    # HfArgumentParser,\n",
    "    # Trainer,\n",
    "    # TrainingArguments,\n",
    "    # default_data_collator,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixture of multually exclusive and independent latent concepts\n",
    "# symmatrical matrices\n",
    "\n",
    "concept1 = np.array([   # positivity bias\n",
    "    [1, 0, 1, 1],  # intention: pos\n",
    "    [0, 0, 0, 0],  # intention: neg\n",
    "    [1, 0, 1, 0],  # result: pos\n",
    "    [1, 0, 0, 1]   # result: neg\n",
    "])\n",
    "\n",
    "concept2 = np.array([   # swing\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 0, 1],\n",
    "    [1, 0, 1, 1],\n",
    "    [0, 1, 1, 1]\n",
    "])\n",
    "\n",
    "concept3 = np.array([   # negativity bias\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 1, 1, 1],\n",
    "    [0, 1, 1, 0],\n",
    "    [0, 1, 0, 1]\n",
    "])\n",
    "\n",
    "concepts = [concept1, concept2, concept3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333 0.33333333]\n",
      "[0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "def normalize(v, ord=1):\n",
    "    v = np.array(v, dtype=float)  # change dtype to floats\n",
    "    norm = np.linalg.norm(v, ord)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    v[v != 0] /= norm\n",
    "    return v\n",
    "\n",
    "\n",
    "# training data gt (TODO: does the model capture this prob distr and use it as prior during inf?)\n",
    "# do not mix concepts when generating training data?\n",
    "probs_train_gt = normalize([1, 1, 1], ord=1)\n",
    "print(probs_train_gt)\n",
    "\n",
    "# testing data gt\n",
    "probs_test_gt = normalize([1, 1, 1], ord=1)\n",
    "print(probs_test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.33333333, 0.66666667, 0.33333333],\n",
       "       [0.33333333, 0.66666667, 0.33333333, 0.66666667],\n",
       "       [0.66666667, 0.33333333, 1.        , 0.33333333],\n",
       "       [0.33333333, 0.66666667, 0.33333333, 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregated gt concept\n",
    "concept_test_gt = sum(prob * concept for prob, concept in zip(probs_test_gt, concepts))\n",
    "concept_test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## event vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = concepts[0].shape[0] + 1\n",
    "n_events = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_state2event(n_states, n_events):\n",
    "\n",
    "    # n_states includes end state (always 0)\n",
    "    # n_events includes end event (\"/\")\n",
    "\n",
    "    state2event = {i:[] for i in range(n_states)}\n",
    "\n",
    "    # state 0 (end) always instantiates as event 0 (\"/\")\n",
    "    state2event[0] = [0]\n",
    "\n",
    "    # other states\n",
    "    for i in range(1, n_events):\n",
    "        state2event[np.random.choice(range(1, n_states))].append(i)\n",
    "\n",
    "    return state2event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0],\n",
       " 1: [5, 6, 7, 17, 18, 22, 23, 24, 26, 33, 35, 43, 46],\n",
       " 2: [4, 8, 10, 12, 25, 27, 36, 44],\n",
       " 3: [3, 13, 14, 16, 19, 20, 21, 29, 30, 32, 37, 38, 39, 48],\n",
       " 4: [1, 2, 9, 11, 15, 28, 31, 34, 40, 41, 42, 45, 47, 49]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate event from latent state\n",
    "# TODO: use different token seqs to denote different states (tokens may be reused across states, but seqs not), instead of single tokens\n",
    "\n",
    "instantiation = gen_state2event(n_states, n_events)\n",
    "instantiation\n",
    "\n",
    "# instantiation = {\n",
    "#     0: [\"/\"],\n",
    "#     1: [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"ab\", \"ac\", \"ad\", \"ae\"],\n",
    "#     2: [\"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"af\", \"ag\", \"ah\", \"ai\", \"aj\", \"ak\", \"al\"],\n",
    "#     3: [\"q\", \"r\", \"s\", \"t\", \"am\", \"an\", \"ao\", \"ap\", \"aq\"],\n",
    "#     4: [\"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"ar\", \"as\", \"at\", \"au\", \"av\", \"aw\", \"ax\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/p-lambda/incontext-learning/blob/84fab2141381001e33b5835e01f4fbf37f34a6a5/generate_data.py#L128\n",
    "def letter_generator(num):\n",
    "    counter = 0\n",
    "    for i in range(1, len(ascii_lowercase)):\n",
    "        for perm in permutations(ascii_lowercase, i):\n",
    "            yield ''.join(perm)\n",
    "            counter += 1\n",
    "            if counter >= num:\n",
    "                return\n",
    "            \n",
    "\n",
    "# generate vocab\n",
    "vocab = list(letter_generator(n_events))\n",
    "# replace delimiters with more interpretable tokens\n",
    "vocab = ['/'] + vocab[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/p-lambda/incontext-learning/blob/84fab2141381001e33b5835e01f4fbf37f34a6a5/generate_data.py#L138\n",
    "def apply_vocab(tokens, vocab):\n",
    "    return [vocab[tok] for tok in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate: concept (transition matrix) >> latent state seq >> instantiate event\n",
    "def gen_obs(concept, end_prob=0.5, sample_length=None):\n",
    "\n",
    "    state_seq = []\n",
    "    event_seq = []\n",
    "\n",
    "    init_state_probs = concept.diagonal()\n",
    "    state_present = init_state_probs > 0\n",
    "    num_states = sum(state_present)\n",
    "\n",
    "    # append a col of end state to the left of concept mtx, with each prob 1/num_states\n",
    "    end_prob = (end_prob * state_present / num_states).reshape(-1, 1)  # convert to shape nx1\n",
    "    concept = np.hstack((end_prob, concept))  # append to the left of concept mtx\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # randomly select a state accroding to their probs to start chaining\n",
    "        state_chosen = np.random.choice(len(init_state_probs), p=normalize(init_state_probs)) + 1  # states are 1-indexed; 0 denotes end state\n",
    "        state_seq.append(state_chosen)\n",
    "        # instantiate event from state\n",
    "        event_seq.append(np.random.choice(instantiation[state_chosen]))\n",
    "\n",
    "        # generate next states/events\n",
    "        while state_chosen != 0:\n",
    "            if sample_length and len(state_seq) >= sample_length: \n",
    "                return state_seq, event_seq\n",
    "            state_probs = concept[state_chosen-1]\n",
    "            state_probs[state_chosen] = 0  # diag of concept mtx denotes presence of states instead of self-self transition prob\n",
    "            state_chosen = np.random.choice(len(state_probs), p=normalize(state_probs))\n",
    "            # print(normalize(state_probs), sum(normalize(state_probs)))\n",
    "            # print('state_chosen', state_chosen, state_probs)\n",
    "            state_seq.append(state_chosen)\n",
    "            event_seq.append(np.random.choice(instantiation[state_chosen]))\n",
    "        \n",
    "        if sample_length is None: \n",
    "            return state_seq, event_seq\n",
    "\n",
    "    # return state_seq, event_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 0, 4, 0, 3, 1, 0, 3, 1, 0, 4],\n",
       " [12, 42, 12, 42, 25, 49, 25, 2, 44, 41, 0, 40, 0, 21, 17, 0, 14, 23, 0, 31])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity\n",
    "concept_test_gt_thres = concept_test_gt.copy()\n",
    "concept_test_gt_thres[concept_test_gt_thres < 0.5] = 0\n",
    "gen_obs(concept_test_gt_thres, end_prob=0.3, sample_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the list of concepts with probs_train_gt to gen_obs\n",
    "# https://github.com/p-lambda/incontext-learning/blob/84fab2141381001e33b5835e01f4fbf37f34a6a5/generate_data.py#L319\n",
    "\n",
    "def gen_samples(num_samples, id_concepts, end_prob, random_data, sample_length=10240):\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for i in tqdm(range(num_samples)):\n",
    "\n",
    "        # randomly select concept to generate obs\n",
    "        j = np.random.choice(len(id_concepts))\n",
    "        # print('j', j)\n",
    "        if not random_data:\n",
    "            h, x = gen_obs(id_concepts[j], end_prob=end_prob, sample_length=sample_length)\n",
    "        \n",
    "        # if generating random_data\n",
    "        else:\n",
    "            h = np.random.randint(low=0, high=n_states, size=sample_length)\n",
    "            x = np.random.randint(low=0, high=len(vocab), size=sample_length)\n",
    "        x = apply_vocab(x, vocab)\n",
    "        samples.append({'text': ' '.join(x), 'concept_idx': j, 'concept_type': 'id', 'hiddens': h})\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:02<00:00, 16.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = gen_samples(num_samples=1000, id_concepts=concepts, end_prob=0.3, random_data=False, sample_length=1024)\n",
    "\n",
    "len(samples[999]['hiddens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(samples, save_path):\n",
    "    df = pd.DataFrame(samples)\n",
    "    df.to_json(save_path, orient='records', lines=True)\n",
    "\n",
    "def samples_to_raw(samples, out_path):\n",
    "    with open(out_path, 'w') as f:\n",
    "        for sample in samples:\n",
    "            f.write(sample['text'] + ' / ')\n",
    "\n",
    "save_as_json(samples, '/net/scratch/shangao/latent-concept/data/train_N1000_L1024_E0.3.json')\n",
    "samples_to_raw(samples, '/net/scratch/shangao/latent-concept/data/train_N1000_L1024_E0.3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
